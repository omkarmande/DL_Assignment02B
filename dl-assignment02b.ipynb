{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb login","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading data set\n!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n!unzip -q nature_12K.zip\n!rm nature_12K.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport wandb\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trying to set device as GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transforming the dataset\nINPUT_SIZE = 224\ntransform = transforms.Compose([\n    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3)\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DataLoaders with 20% validation split of train\ndef get_dataloaders(train_dir, test_dir, batch_size=64):\n    full = datasets.ImageFolder(train_dir, transform=transform)\n    targets = [label for _, label in full.samples]\n    train_idx, val_idx = train_test_split(\n        list(range(len(full))), test_size=0.2, stratify=targets, random_state=42\n    )\n    train_loader = DataLoader(Subset(full, train_idx), batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader   = DataLoader(Subset(full, val_idx),   batch_size=batch_size, shuffle=False, num_workers=2)\n\n    test_ds = datasets.ImageFolder(test_dir, transform=transform)\n    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Building ResNet50 with with 10 classes in last layer\ndef build_model(num_classes=10, pretrained=True):\n    weights = models.ResNet50_Weights.DEFAULT if pretrained else None\n    model = models.resnet50(weights=weights)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply one of four strategies\ndef apply_strategy(model, strategy, k=None):\n    # Strategy 1: freeze all but final fc\n    if strategy == \"freeze_all_except_last\":\n        for p in model.parameters():\n            p.requires_grad = False\n        for p in model.fc.parameters():\n            p.requires_grad = True\n\n    # Strategy 2: freeze first k children modules\n    elif strategy == \"freeze_first_k\":\n        children = list(model.children())\n        for idx, child in enumerate(children):\n            requires_grad = False if idx < k else True\n            for p in child.parameters():\n                p.requires_grad = requires_grad\n\n    # Strategy 3: freeze last k children modules (excluding fc)\n    elif strategy == \"freeze_last_k\":\n        children = list(model.children())[:-1]  # omiting final fc\n        total = len(children)\n        for idx, child in enumerate(children):\n            requires_grad = False if idx >= total - k else True\n            for p in child.parameters():\n                p.requires_grad = requires_grad\n        # ensure fc is trainable\n        for p in model.fc.parameters():\n            p.requires_grad = True\n\n    # Strategy 4: train from scratch (nothing frozen)\n    elif strategy == \"train_from_scratch\":\n        for p in model.parameters():\n            p.requires_grad = True\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\ndef train_one_epoch(model, opt, criterion, loader):\n    model.train()\n    running = 0.0\n    for X, y in loader:\n        X, y = X.to(device), y.to(device)\n        opt.zero_grad()\n        out = model(X)\n        loss = criterion(out, y)\n        loss.backward()\n        opt.step()\n        running += loss.item()\n    return running / len(loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on a loader\ndef evaluate(model, criterion, loader, tag):\n    model.eval()\n    total, correct, loss_sum = 0, 0, 0.0\n    with torch.no_grad():\n        for X, y in loader:\n            X, y = X.to(device), y.to(device)\n            out = model(X)\n            loss = criterion(out, y)\n            loss_sum += loss.item()\n            pred = out.argmax(dim=1)\n            correct += (pred == y).sum().item()\n            total += y.size(0)\n    acc = 100 * correct / total\n    wandb.log({f\"{tag}_loss\": loss_sum/len(loader), f\"{tag}_acc\": acc})\n    print(f\"{tag} â€” loss: {loss_sum/len(loader):.4f}, acc: {acc:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main loop over strategies\nif __name__ == \"__main__\":\n    train_dl, val_dl, test_dl = get_dataloaders(\n        \"inaturalist_12K/train\", \"inaturalist_12K/val\", batch_size=64\n    )\n\n    strategies = [\n        (\"freeze_all_except_last\", None),\n        (\"freeze_first_k\", 4),   # freeze up to just before layer1\n        (\"freeze_last_k\", 2),    # freeze layer4 and avgpool\n        (\"train_from_scratch\", None)\n    ]\n\n    for name, k in strategies:\n        wandb.init(project=\"Assignment_02B\", name=name, reinit=True)\n        print(f\"\\n=== Strategy: {name}, k={k} ===\")\n\n        # Build and apply\n        pretrained = False if name == \"train_from_scratch\" else True\n        model = build_model(pretrained=pretrained).to(device)\n        model = apply_strategy(model, name, k)\n        model = nn.DataParallel(model)\n\n        # Optimizer on trainable params only\n        optimizer = optim.NAdam(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            lr=1e-4, weight_decay=0.005\n        )\n        criterion = nn.CrossEntropyLoss()\n\n        # Train & validate\n        for epoch in range(1, 11):\n            train_loss = train_one_epoch(model, optimizer, criterion, train_dl)\n            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss})\n            evaluate(model, criterion, val_dl, \"val\")\n\n        # Final test evaluation\n        evaluate(model, criterion, test_dl, \"test\")\n\n        wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}